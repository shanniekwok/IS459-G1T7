{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import col, dayofweek, when, month, hour, lag, avg, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "|       day|    LCLid|       energy_median|         energy_mean|          energy_max|energy_count|          energy_std|        energy_sum|          energy_min|temperatureMax| temperatureMaxTime|windBearing|             icon|dewPoint| temperatureMinTime|cloudCover|windSpeed|pressure|apparentTemperatureMinTime|apparentTemperatureHigh|precipType|visibility|humidity|apparentTemperatureHighTime|apparentTemperatureLow|apparentTemperatureMax|uvIndex|               time|         sunsetTime|temperatureLow|temperatureMin|temperatureHigh|        sunriseTime|temperatureHighTime|        uvIndexTime|             summary| temperatureLowTime|apparentTemperatureMin|apparentTemperatureMaxTime|apparentTemperatureLowTime|moonPhase|Type|\n",
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "|2013-04-14|MAC004175|              0.0145|0.013020833333333344|0.018000000000000002|          48|0.003442473030740...|0.6250000000000004|0.006999999999999999|         15.43|2013-04-15 20:00:00|        220|partly-cloudy-day|     7.8|2013-04-16 06:00:00|      0.42|      5.4| 1016.86|       2013-04-16 06:00:00|                  15.43|      rain|     12.96|    0.72|        2013-04-15 20:00:00|                  7.26|                 15.43|    4.0|2013-04-15 07:00:00|2013-04-16 02:57:32|          9.63|         10.08|          15.43|2013-04-15 13:05:30|2013-04-15 20:00:00|2013-04-15 20:00:00|Partly cloudy unt...|2013-04-16 09:00:00|                 10.08|       2013-04-15 20:00:00|       2013-04-16 10:00:00|     0.16|NULL|\n",
      "|2013-04-15|MAC004175|               0.016| 0.03004166666666666|               0.126|          48|0.036537918708460924|1.4419999999999995|0.006999999999999999|         16.55|2013-04-16 23:00:00|        218|partly-cloudy-day|    6.83|2013-04-16 09:00:00|       0.4|     5.75|  1015.5|       2013-04-16 10:00:00|                  16.55|      rain|     12.52|     0.7|        2013-04-16 23:00:00|                  8.26|                 16.55|    4.0|2013-04-16 07:00:00|2013-04-17 02:59:13|          9.66|          9.63|          16.55|2013-04-16 13:03:22|2013-04-16 23:00:00|2013-04-16 19:00:00|Partly cloudy unt...|2013-04-17 09:00:00|                  7.26|       2013-04-16 23:00:00|       2013-04-17 09:00:00|     0.19|NULL|\n",
      "|2013-04-16|MAC004175|               0.016| 0.04162499999999999|               0.536|          48| 0.08862486683361841|             1.998|               0.008|         17.95|2013-04-18 01:00:00|        203|             wind|    9.18|2013-04-17 09:00:00|      0.54|     5.53| 1014.48|       2013-04-17 09:00:00|                  17.95|      rain|      10.8|    0.77|        2013-04-18 01:00:00|                  5.39|                 17.95|    4.0|2013-04-17 07:00:00|2013-04-18 03:00:53|          9.13|          9.66|          17.95|2013-04-17 13:01:14|2013-04-18 01:00:00|2013-04-17 20:00:00|Partly cloudy thr...|2013-04-18 13:00:00|                  8.26|       2013-04-18 01:00:00|       2013-04-18 13:00:00|     0.22|NULL|\n",
      "|2013-04-17|MAC004175|              0.0165|0.052062500000000005|               0.123|          48| 0.04638204267794513|             2.499|0.006999999999999999|         14.11|2013-04-18 21:00:00|        237|             wind|    4.05|2013-04-19 06:00:00|      0.35|     8.12|  1013.1|       2013-04-19 06:00:00|                  14.11|      rain|     12.81|    0.64|        2013-04-18 21:00:00|                  3.88|                 14.11|    4.0|2013-04-18 07:00:00|2013-04-19 03:02:34|          7.08|          8.48|          14.11|2013-04-18 12:59:07|2013-04-18 21:00:00|2013-04-18 19:00:00|Breezy until afte...|2013-04-19 10:00:00|                  5.27|       2013-04-18 21:00:00|       2013-04-19 10:00:00|     0.25|NULL|\n",
      "|2013-04-18|MAC004175|0.026500000000000003| 0.09245833333333336|               0.879|          48|  0.1687700132603139|4.4380000000000015|0.006999999999999999|         11.95|2013-04-20 01:00:00|        301|partly-cloudy-day|    3.68|2013-04-20 06:00:00|      0.47|     2.33| 1025.56|       2013-04-19 10:00:00|                  11.95|      rain|     13.12|    0.69|        2013-04-20 01:00:00|                  2.88|                 11.95|    4.0|2013-04-19 07:00:00|2013-04-20 03:04:14|          2.88|          6.17|          11.95|2013-04-19 12:57:01|2013-04-20 01:00:00|2013-04-19 20:00:00|Partly cloudy unt...|2013-04-20 13:00:00|                  3.88|       2013-04-20 01:00:00|       2013-04-20 13:00:00|     0.28|NULL|\n",
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- day: date (nullable = true)\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- energy_median: double (nullable = true)\n",
      " |-- energy_mean: double (nullable = true)\n",
      " |-- energy_max: double (nullable = true)\n",
      " |-- energy_count: integer (nullable = true)\n",
      " |-- energy_std: double (nullable = true)\n",
      " |-- energy_sum: double (nullable = true)\n",
      " |-- energy_min: double (nullable = true)\n",
      " |-- temperatureMax: double (nullable = true)\n",
      " |-- temperatureMaxTime: timestamp (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- temperatureMinTime: timestamp (nullable = true)\n",
      " |-- cloudCover: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperatureMinTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureHigh: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- apparentTemperatureHighTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureLow: double (nullable = true)\n",
      " |-- apparentTemperatureMax: double (nullable = true)\n",
      " |-- uvIndex: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- sunsetTime: timestamp (nullable = true)\n",
      " |-- temperatureLow: double (nullable = true)\n",
      " |-- temperatureMin: double (nullable = true)\n",
      " |-- temperatureHigh: double (nullable = true)\n",
      " |-- sunriseTime: timestamp (nullable = true)\n",
      " |-- temperatureHighTime: timestamp (nullable = true)\n",
      " |-- uvIndexTime: timestamp (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- temperatureLowTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureMin: double (nullable = true)\n",
      " |-- apparentTemperatureMaxTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureLowTime: timestamp (nullable = true)\n",
      " |-- moonPhase: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ParquetViewer\").config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.memory\", \"4g\").getOrCreate()\n",
    "\n",
    "# Load local Parquet file\n",
    "df = spark.read.parquet(\"/Users/caitlinyap/GitHub/IS459-G1T7/ML/merged_df1_df3_df7_df8/\")\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.show(5)\n",
    "\n",
    "# Print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------+-----------+----------+------------+----------+----------+----------+--------------+------------------+-----------+-----+--------+------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-----+----------+--------------+--------------+---------------+-----------+-------------------+-----------+-------+------------------+----------------------+--------------------------+--------------------------+---------+-------+\n",
      "|day|LCLid|energy_median|energy_mean|energy_max|energy_count|energy_std|energy_sum|energy_min|temperatureMax|temperatureMaxTime|windBearing| icon|dewPoint|temperatureMinTime|cloudCover|windSpeed|pressure|apparentTemperatureMinTime|apparentTemperatureHigh|precipType|visibility|humidity|apparentTemperatureHighTime|apparentTemperatureLow|apparentTemperatureMax|uvIndex| time|sunsetTime|temperatureLow|temperatureMin|temperatureHigh|sunriseTime|temperatureHighTime|uvIndexTime|summary|temperatureLowTime|apparentTemperatureMin|apparentTemperatureMaxTime|apparentTemperatureLowTime|moonPhase|   Type|\n",
      "+---+-----+-------------+-----------+----------+------------+----------+----------+----------+--------------+------------------+-----------+-----+--------+------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-----+----------+--------------+--------------+---------------+-----------+-------------------+-----------+-------+------------------+----------------------+--------------------------+--------------------------+---------+-------+\n",
      "|  0|    0|           30|         30|        30|           0|     11339|        30|        30|         10714|             10714|      10714|10714|   10714|             10714|     15813|    10714|   10714|                     10714|                  10714|     10714|     10714|   10714|                      10714|                 10714|                 10714|  15813|10714|     10714|         10714|         10714|          10714|      10714|              10714|      15813|  10714|             10714|                 10714|                     10714|                     10714|    10714|3436401|\n",
      "+---+-----+-------------+-----------+----------+------------+----------+----------+----------+--------------+------------------+-----------+-----+--------+------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-----+----------+--------------+--------------+---------------+-----------+-------------------+-----------+-------+------------------+----------------------+--------------------------+--------------------------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count missing values in each column\n",
    "missing_values = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after cleaning: 3517002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Time-based features\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"day\")))\n",
    "df = df.withColumn(\"is_weekend\", when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), 1).otherwise(0))\n",
    "df = df.withColumn(\"month\", month(col(\"day\")))\n",
    "df = df.withColumn(\"hour\", hour(col(\"time\")))\n",
    "\n",
    "# Lag feature (previous day's energy consumption)\n",
    "window_spec = Window.partitionBy(\"LCLid\").orderBy(\"day\")\n",
    "df = df.withColumn(\"lag_1_day\", lag(\"energy_sum\", 1).over(window_spec))\n",
    "\n",
    "# Weather-based features\n",
    "df = df.withColumn(\"temperature_variability\", col(\"temperatureMax\") - col(\"temperatureMin\"))\n",
    "df = df.withColumn(\"humidity_temp_interaction\", col(\"humidity\") * col(\"temperatureMax\"))\n",
    "\n",
    "# Precipitation & Cloud Cover features\n",
    "df = df.withColumn(\"cloud_temp_interaction\", col(\"cloudCover\") * col(\"temperatureMax\"))\n",
    "\n",
    "# Rolling sum of precipitation over 7 days\n",
    "df = df.withColumn(\"rolling_precipitation_7d\", spark_sum(\"precipType\").over(window_spec.rowsBetween(-6, 0)))\n",
    "\n",
    "# Drop only rows where `energy_sum` is NULL\n",
    "df = df.dropna(subset=[\"energy_sum\"])\n",
    "\n",
    "# Fill missing values instead of dropping\n",
    "df = df.fillna({\n",
    "    \"lag_1_day\": 0,  \n",
    "    \"temperature_variability\": df.select(avg(\"temperatureMax\") - avg(\"temperatureMin\")).collect()[0][0],\n",
    "    \"humidity_temp_interaction\": df.select(avg(\"humidity\") * avg(\"temperatureMax\")).collect()[0][0],\n",
    "    \"cloud_temp_interaction\": df.select(avg(\"cloudCover\") * avg(\"temperatureMax\")).collect()[0][0],\n",
    "    \"rolling_precipitation_7d\": 0  \n",
    "})\n",
    "\n",
    "# Drop columns with too many missing values\n",
    "columns_to_drop = [\"precipType\", \"summary\", \"Type\"]\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "# Verify that we still have rows\n",
    "print(f\"Dataset size after cleaning: {df.count()}\")\n",
    "\n",
    "# Assemble features\n",
    "feature_cols = [\n",
    "    \"day_of_week\", \"is_weekend\", \"lag_1_day\",\n",
    "    \"temperature_variability\", \"humidity_temp_interaction\",\n",
    "    \"cloud_temp_interaction\", \"rolling_precipitation_7d\"\n",
    "]\n",
    "\n",
    "if \"features\" in df.columns:\n",
    "    df = df.drop(\"features\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3165126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 351876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train-Test Split\n",
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "# Verify train-test sizes\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "\n",
    "# Stop training if dataset is empty\n",
    "if train_df.count() == 0:\n",
    "    raise ValueError(\"Training dataset is empty. Check preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3165126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 351876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-core_2.12-3.5.5.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 66:==============================================>          (9 + 2) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 4.627385526912342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Train Model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"energy_sum\", numTrees=100)\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Predictions\n",
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = RegressionEvaluator(labelCol=\"energy_sum\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "os.environ[\"DMLC_TRACKER_URI\"] = \"127.0.0.1\"\n",
    "os.environ[\"DMLC_TRACKER_PORT\"] = \"9091\"\n",
    "os.environ[\"DMLC_NUM_WORKER\"] = \"1\"\n",
    "os.environ[\"DMLC_NUM_SERVER\"] = \"1\"\n",
    "\n",
    "train_df_xgb, test_df_xgb = df.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 01:56:31,155 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 1 workers with\n",
      "\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'max_depth': 10, 'subsample': 0.8, 'num_round': 150, 'eta': 0.05, 'n_workers': 1, 'use_external_storage': False, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-03-13 01:57:17,543 INFO XGBoost-PySpark: _train_booster Training on CPUs 1]\n",
      "[01:57:18] Task 0 got rank 0\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [01:57:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"n_workers\", \"num_round\", \"use_external_storage\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "2025-03-13 01:57:54,180 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-03-13 01:58:47,725 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "[Stage 61:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (XGBoost): 3.9490515479235446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train XGBoost Model\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"scaled_features\",\n",
    "    label_col=\"energy_sum\",\n",
    "    max_depth=10,\n",
    "    eta=0.05,\n",
    "    subsample=0.8,\n",
    "    # num_round=150,\n",
    "    # n_workers=1,  # Run in single-worker mode\n",
    "    # use_external_storage=False\n",
    ")\n",
    "\n",
    "xgb_model = xgb.fit(train_df_xgb)\n",
    "\n",
    "# Make Predictions\n",
    "predictions = xgb_model.transform(test_df_xgb)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "evaluator = RegressionEvaluator(labelCol=\"energy_sum\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Test RMSE (XGBoost): {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
