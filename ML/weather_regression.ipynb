{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import col, dayofweek, when, month, hour, lag, avg, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "|       day|    LCLid|       energy_median|         energy_mean|          energy_max|energy_count|          energy_std|        energy_sum|          energy_min|temperatureMax| temperatureMaxTime|windBearing|             icon|dewPoint| temperatureMinTime|cloudCover|windSpeed|pressure|apparentTemperatureMinTime|apparentTemperatureHigh|precipType|visibility|humidity|apparentTemperatureHighTime|apparentTemperatureLow|apparentTemperatureMax|uvIndex|               time|         sunsetTime|temperatureLow|temperatureMin|temperatureHigh|        sunriseTime|temperatureHighTime|        uvIndexTime|             summary| temperatureLowTime|apparentTemperatureMin|apparentTemperatureMaxTime|apparentTemperatureLowTime|moonPhase|Type|\n",
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "|2013-04-14|MAC004175|              0.0145|0.013020833333333344|0.018000000000000002|          48|0.003442473030740...|0.6250000000000004|0.006999999999999999|         15.43|2013-04-15 20:00:00|        220|partly-cloudy-day|     7.8|2013-04-16 06:00:00|      0.42|      5.4| 1016.86|       2013-04-16 06:00:00|                  15.43|      rain|     12.96|    0.72|        2013-04-15 20:00:00|                  7.26|                 15.43|    4.0|2013-04-15 07:00:00|2013-04-16 02:57:32|          9.63|         10.08|          15.43|2013-04-15 13:05:30|2013-04-15 20:00:00|2013-04-15 20:00:00|Partly cloudy unt...|2013-04-16 09:00:00|                 10.08|       2013-04-15 20:00:00|       2013-04-16 10:00:00|     0.16|NULL|\n",
      "|2013-04-15|MAC004175|               0.016| 0.03004166666666666|               0.126|          48|0.036537918708460924|1.4419999999999995|0.006999999999999999|         16.55|2013-04-16 23:00:00|        218|partly-cloudy-day|    6.83|2013-04-16 09:00:00|       0.4|     5.75|  1015.5|       2013-04-16 10:00:00|                  16.55|      rain|     12.52|     0.7|        2013-04-16 23:00:00|                  8.26|                 16.55|    4.0|2013-04-16 07:00:00|2013-04-17 02:59:13|          9.66|          9.63|          16.55|2013-04-16 13:03:22|2013-04-16 23:00:00|2013-04-16 19:00:00|Partly cloudy unt...|2013-04-17 09:00:00|                  7.26|       2013-04-16 23:00:00|       2013-04-17 09:00:00|     0.19|NULL|\n",
      "|2013-04-16|MAC004175|               0.016| 0.04162499999999999|               0.536|          48| 0.08862486683361841|             1.998|               0.008|         17.95|2013-04-18 01:00:00|        203|             wind|    9.18|2013-04-17 09:00:00|      0.54|     5.53| 1014.48|       2013-04-17 09:00:00|                  17.95|      rain|      10.8|    0.77|        2013-04-18 01:00:00|                  5.39|                 17.95|    4.0|2013-04-17 07:00:00|2013-04-18 03:00:53|          9.13|          9.66|          17.95|2013-04-17 13:01:14|2013-04-18 01:00:00|2013-04-17 20:00:00|Partly cloudy thr...|2013-04-18 13:00:00|                  8.26|       2013-04-18 01:00:00|       2013-04-18 13:00:00|     0.22|NULL|\n",
      "|2013-04-17|MAC004175|              0.0165|0.052062500000000005|               0.123|          48| 0.04638204267794513|             2.499|0.006999999999999999|         14.11|2013-04-18 21:00:00|        237|             wind|    4.05|2013-04-19 06:00:00|      0.35|     8.12|  1013.1|       2013-04-19 06:00:00|                  14.11|      rain|     12.81|    0.64|        2013-04-18 21:00:00|                  3.88|                 14.11|    4.0|2013-04-18 07:00:00|2013-04-19 03:02:34|          7.08|          8.48|          14.11|2013-04-18 12:59:07|2013-04-18 21:00:00|2013-04-18 19:00:00|Breezy until afte...|2013-04-19 10:00:00|                  5.27|       2013-04-18 21:00:00|       2013-04-19 10:00:00|     0.25|NULL|\n",
      "|2013-04-18|MAC004175|0.026500000000000003| 0.09245833333333336|               0.879|          48|  0.1687700132603139|4.4380000000000015|0.006999999999999999|         11.95|2013-04-20 01:00:00|        301|partly-cloudy-day|    3.68|2013-04-20 06:00:00|      0.47|     2.33| 1025.56|       2013-04-19 10:00:00|                  11.95|      rain|     13.12|    0.69|        2013-04-20 01:00:00|                  2.88|                 11.95|    4.0|2013-04-19 07:00:00|2013-04-20 03:04:14|          2.88|          6.17|          11.95|2013-04-19 12:57:01|2013-04-20 01:00:00|2013-04-19 20:00:00|Partly cloudy unt...|2013-04-20 13:00:00|                  3.88|       2013-04-20 01:00:00|       2013-04-20 13:00:00|     0.28|NULL|\n",
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- day: date (nullable = true)\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- energy_median: double (nullable = true)\n",
      " |-- energy_mean: double (nullable = true)\n",
      " |-- energy_max: double (nullable = true)\n",
      " |-- energy_count: integer (nullable = true)\n",
      " |-- energy_std: double (nullable = true)\n",
      " |-- energy_sum: double (nullable = true)\n",
      " |-- energy_min: double (nullable = true)\n",
      " |-- temperatureMax: double (nullable = true)\n",
      " |-- temperatureMaxTime: timestamp (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- temperatureMinTime: timestamp (nullable = true)\n",
      " |-- cloudCover: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperatureMinTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureHigh: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- apparentTemperatureHighTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureLow: double (nullable = true)\n",
      " |-- apparentTemperatureMax: double (nullable = true)\n",
      " |-- uvIndex: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- sunsetTime: timestamp (nullable = true)\n",
      " |-- temperatureLow: double (nullable = true)\n",
      " |-- temperatureMin: double (nullable = true)\n",
      " |-- temperatureHigh: double (nullable = true)\n",
      " |-- sunriseTime: timestamp (nullable = true)\n",
      " |-- temperatureHighTime: timestamp (nullable = true)\n",
      " |-- uvIndexTime: timestamp (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- temperatureLowTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureMin: double (nullable = true)\n",
      " |-- apparentTemperatureMaxTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureLowTime: timestamp (nullable = true)\n",
      " |-- moonPhase: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ParquetViewer\").config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.memory\", \"4g\").getOrCreate()\n",
    "\n",
    "# Load Parquet file\n",
    "repo_root = os.getcwd()  \n",
    "parquet_path = os.path.join(repo_root, \"merged_df1_df3_df7_df8\")\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------+-----------+----------+------------+----------+----------+----------+--------------+------------------+-----------+-----+--------+------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-----+----------+--------------+--------------+---------------+-----------+-------------------+-----------+-------+------------------+----------------------+--------------------------+--------------------------+---------+-------+\n",
      "|day|LCLid|energy_median|energy_mean|energy_max|energy_count|energy_std|energy_sum|energy_min|temperatureMax|temperatureMaxTime|windBearing| icon|dewPoint|temperatureMinTime|cloudCover|windSpeed|pressure|apparentTemperatureMinTime|apparentTemperatureHigh|precipType|visibility|humidity|apparentTemperatureHighTime|apparentTemperatureLow|apparentTemperatureMax|uvIndex| time|sunsetTime|temperatureLow|temperatureMin|temperatureHigh|sunriseTime|temperatureHighTime|uvIndexTime|summary|temperatureLowTime|apparentTemperatureMin|apparentTemperatureMaxTime|apparentTemperatureLowTime|moonPhase|   Type|\n",
      "+---+-----+-------------+-----------+----------+------------+----------+----------+----------+--------------+------------------+-----------+-----+--------+------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-----+----------+--------------+--------------+---------------+-----------+-------------------+-----------+-------+------------------+----------------------+--------------------------+--------------------------+---------+-------+\n",
      "|  0|    0|           30|         30|        30|           0|     11339|        30|        30|         10714|             10714|      10714|10714|   10714|             10714|     15813|    10714|   10714|                     10714|                  10714|     10714|     10714|   10714|                      10714|                 10714|                 10714|  15813|10714|     10714|         10714|         10714|          10714|      10714|              10714|      15813|  10714|             10714|                 10714|                     10714|                     10714|    10714|3436401|\n",
      "+---+-----+-------------+-----------+----------+------------+----------+----------+----------+--------------+------------------+-----------+-----+--------+------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-----+----------+--------------+--------------+---------------+-----------+-------------------+-----------+-------+------------------+----------------------+--------------------------+--------------------------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col, sum\n",
    "\n",
    "# # Count missing values in each column\n",
    "# missing_values = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "# missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after cleaning: 3517002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Time-based features\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"day\")))\n",
    "df = df.withColumn(\"is_weekend\", when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), 1).otherwise(0))\n",
    "df = df.withColumn(\"month\", month(col(\"day\")))\n",
    "df = df.withColumn(\"hour\", hour(col(\"time\")))\n",
    "\n",
    "# Lag feature (previous day's energy consumption)\n",
    "window_spec = Window.partitionBy(\"LCLid\").orderBy(\"day\")\n",
    "df = df.withColumn(\"lag_1_day\", lag(\"energy_sum\", 1).over(window_spec))\n",
    "\n",
    "# Weather-based features\n",
    "df = df.withColumn(\"temperature_variability\", col(\"temperatureMax\") - col(\"temperatureMin\"))\n",
    "df = df.withColumn(\"humidity_temp_interaction\", col(\"humidity\") * col(\"temperatureMax\"))\n",
    "df = df.withColumn(\"cloud_temp_interaction\", col(\"cloudCover\") * col(\"temperatureMax\"))\n",
    "\n",
    "# Rolling sum of precipitation over 7 days\n",
    "df = df.withColumn(\"rolling_precipitation_7d\", spark_sum(\"precipType\").over(window_spec.rowsBetween(-6, 0)))\n",
    "\n",
    "# Drop only rows where `energy_sum` is NULL\n",
    "df = df.dropna(subset=[\"energy_sum\"])\n",
    "\n",
    "# Fill missing values instead of dropping\n",
    "df = df.fillna({\n",
    "    \"lag_1_day\": 0,  \n",
    "    \"temperature_variability\": df.select(avg(\"temperatureMax\") - avg(\"temperatureMin\")).collect()[0][0],\n",
    "    \"humidity_temp_interaction\": df.select(avg(\"humidity\") * avg(\"temperatureMax\")).collect()[0][0],\n",
    "    \"cloud_temp_interaction\": df.select(avg(\"cloudCover\") * avg(\"temperatureMax\")).collect()[0][0],\n",
    "    \"rolling_precipitation_7d\": 0  \n",
    "})\n",
    "\n",
    "# Drop columns with too many missing values\n",
    "columns_to_drop = [\"precipType\", \"summary\", \"Type\"]\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "# Verify that we still have rows\n",
    "print(f\"Dataset size after cleaning: {df.count()}\")\n",
    "\n",
    "# Assemble features\n",
    "feature_cols = [\n",
    "    \"day_of_week\", \"is_weekend\", \"lag_1_day\",\n",
    "    \"temperature_variability\", \"humidity_temp_interaction\",\n",
    "    \"cloud_temp_interaction\", \"rolling_precipitation_7d\"\n",
    "]\n",
    "\n",
    "if \"features\" in df.columns:\n",
    "    df = df.drop(\"features\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3165126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 351876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "\n",
    "if train_df.count() == 0:\n",
    "    raise ValueError(\"Training dataset is empty. Check preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 4.7671459655217\n",
      "RMSE as percentage of mean: 45.62%\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"energy_sum\", numTrees=100)\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "predictions = rf_model.transform(test_df)\n",
    "predictions = predictions.orderBy(\"day\", \"LCLid\")\n",
    "\n",
    "# save predictions in csv\n",
    "repo_root = os.getcwd()  \n",
    "output_folder = os.path.join(repo_root, \"random_forest_output\")  \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "temp_output_folder = os.path.join(repo_root, \"temp_output\")\n",
    "predictions.select(\"day\", \"LCLid\", \"energy_sum\", \"prediction\").repartition(1).write.mode(\"overwrite\").csv(temp_output_folder, header=True)\n",
    "part_file = glob.glob(os.path.join(temp_output_folder, \"part-*.csv\"))[0]\n",
    "final_output_path = os.path.join(output_folder, \"random_forest_predictions.csv\")\n",
    "shutil.move(part_file, final_output_path)\n",
    "shutil.rmtree(temp_output_folder)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"energy_sum\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "mean_energy_sum = df.select(avg(\"energy_sum\")).collect()[0][0]\n",
    "rmse_percentage = (4.62 / mean_energy_sum) * 100\n",
    "print(f\"RMSE as percentage of mean: {rmse_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "os.environ[\"DMLC_TRACKER_URI\"] = \"127.0.0.1\"\n",
    "os.environ[\"DMLC_TRACKER_PORT\"] = \"9091\"\n",
    "os.environ[\"DMLC_NUM_WORKER\"] = \"1\"\n",
    "os.environ[\"DMLC_NUM_SERVER\"] = \"1\"\n",
    "\n",
    "train_df_xgb, test_df_xgb = df.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:XGBoost-PySpark:Running xgboost-2.1.4 on 1 workers with       (9 + 3) / 12]\n",
      "\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'max_depth': 10, 'subsample': 0.8, 'eta': 0.05, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-03-16 00:41:50,304 INFO XGBoost-PySpark: _train_booster Training on CPUs 1]\n",
      "[00:41:51] Task 0 got rank 0\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!                                 \n",
      "2025-03-16 00:42:43,113 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "2025-03-16 00:43:14,997 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2025-03-16 00:44:03,437 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2025-03-16 00:44:27,318 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (XGBoost): 3.949051547923543\n",
      "RMSE as percentage of mean: 45.62%\n"
     ]
    }
   ],
   "source": [
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"scaled_features\",\n",
    "    label_col=\"energy_sum\",\n",
    "    max_depth=10,\n",
    "    eta=0.05,\n",
    "    subsample=0.8,\n",
    "    # num_round=150,\n",
    "    # n_workers=1,  # Run in single-worker mode\n",
    "    # use_external_storage=False\n",
    ")\n",
    "\n",
    "xgb_model = xgb.fit(train_df_xgb)\n",
    "\n",
    "predictions = xgb_model.transform(test_df_xgb)\n",
    "predictions = predictions.orderBy(\"day\", \"LCLid\")\n",
    "\n",
    "# save predictions in csv\n",
    "repo_root = os.getcwd()  \n",
    "output_folder = os.path.join(repo_root, \"xgboost_output\")  \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "temp_output_folder = os.path.join(repo_root, \"temp_output\")\n",
    "predictions.select(\"day\", \"LCLid\", \"energy_sum\", \"prediction\").repartition(1).write.mode(\"overwrite\").csv(temp_output_folder, header=True)\n",
    "part_file = glob.glob(os.path.join(temp_output_folder, \"part-*.csv\"))[0]\n",
    "final_output_path = os.path.join(output_folder, \"xgboost_predictions.csv\")\n",
    "shutil.move(part_file, final_output_path)\n",
    "shutil.rmtree(temp_output_folder)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"energy_sum\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Test RMSE (XGBoost): {rmse}\")\n",
    "mean_energy_sum = df.select(avg(\"energy_sum\")).collect()[0][0]\n",
    "rmse_percentage = (4.62 / mean_energy_sum) * 100\n",
    "print(f\"RMSE as percentage of mean: {rmse_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latest XGBoost Model (after hyperparamter tuning)\n",
    "#### RMSE = 0.97 (high accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "|       day|    LCLid|       energy_median|         energy_mean|          energy_max|energy_count|          energy_std|        energy_sum|          energy_min|temperatureMax| temperatureMaxTime|windBearing|             icon|dewPoint| temperatureMinTime|cloudCover|windSpeed|pressure|apparentTemperatureMinTime|apparentTemperatureHigh|precipType|visibility|humidity|apparentTemperatureHighTime|apparentTemperatureLow|apparentTemperatureMax|uvIndex|               time|         sunsetTime|temperatureLow|temperatureMin|temperatureHigh|        sunriseTime|temperatureHighTime|        uvIndexTime|             summary| temperatureLowTime|apparentTemperatureMin|apparentTemperatureMaxTime|apparentTemperatureLowTime|moonPhase|Type|\n",
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "|2013-04-14|MAC004175|              0.0145|0.013020833333333344|0.018000000000000002|          48|0.003442473030740...|0.6250000000000004|0.006999999999999999|         15.43|2013-04-15 20:00:00|        220|partly-cloudy-day|     7.8|2013-04-16 06:00:00|      0.42|      5.4| 1016.86|       2013-04-16 06:00:00|                  15.43|      rain|     12.96|    0.72|        2013-04-15 20:00:00|                  7.26|                 15.43|    4.0|2013-04-15 07:00:00|2013-04-16 02:57:32|          9.63|         10.08|          15.43|2013-04-15 13:05:30|2013-04-15 20:00:00|2013-04-15 20:00:00|Partly cloudy unt...|2013-04-16 09:00:00|                 10.08|       2013-04-15 20:00:00|       2013-04-16 10:00:00|     0.16|NULL|\n",
      "|2013-04-15|MAC004175|               0.016| 0.03004166666666666|               0.126|          48|0.036537918708460924|1.4419999999999995|0.006999999999999999|         16.55|2013-04-16 23:00:00|        218|partly-cloudy-day|    6.83|2013-04-16 09:00:00|       0.4|     5.75|  1015.5|       2013-04-16 10:00:00|                  16.55|      rain|     12.52|     0.7|        2013-04-16 23:00:00|                  8.26|                 16.55|    4.0|2013-04-16 07:00:00|2013-04-17 02:59:13|          9.66|          9.63|          16.55|2013-04-16 13:03:22|2013-04-16 23:00:00|2013-04-16 19:00:00|Partly cloudy unt...|2013-04-17 09:00:00|                  7.26|       2013-04-16 23:00:00|       2013-04-17 09:00:00|     0.19|NULL|\n",
      "|2013-04-16|MAC004175|               0.016| 0.04162499999999999|               0.536|          48| 0.08862486683361841|             1.998|               0.008|         17.95|2013-04-18 01:00:00|        203|             wind|    9.18|2013-04-17 09:00:00|      0.54|     5.53| 1014.48|       2013-04-17 09:00:00|                  17.95|      rain|      10.8|    0.77|        2013-04-18 01:00:00|                  5.39|                 17.95|    4.0|2013-04-17 07:00:00|2013-04-18 03:00:53|          9.13|          9.66|          17.95|2013-04-17 13:01:14|2013-04-18 01:00:00|2013-04-17 20:00:00|Partly cloudy thr...|2013-04-18 13:00:00|                  8.26|       2013-04-18 01:00:00|       2013-04-18 13:00:00|     0.22|NULL|\n",
      "|2013-04-17|MAC004175|              0.0165|0.052062500000000005|               0.123|          48| 0.04638204267794513|             2.499|0.006999999999999999|         14.11|2013-04-18 21:00:00|        237|             wind|    4.05|2013-04-19 06:00:00|      0.35|     8.12|  1013.1|       2013-04-19 06:00:00|                  14.11|      rain|     12.81|    0.64|        2013-04-18 21:00:00|                  3.88|                 14.11|    4.0|2013-04-18 07:00:00|2013-04-19 03:02:34|          7.08|          8.48|          14.11|2013-04-18 12:59:07|2013-04-18 21:00:00|2013-04-18 19:00:00|Breezy until afte...|2013-04-19 10:00:00|                  5.27|       2013-04-18 21:00:00|       2013-04-19 10:00:00|     0.25|NULL|\n",
      "|2013-04-18|MAC004175|0.026500000000000003| 0.09245833333333336|               0.879|          48|  0.1687700132603139|4.4380000000000015|0.006999999999999999|         11.95|2013-04-20 01:00:00|        301|partly-cloudy-day|    3.68|2013-04-20 06:00:00|      0.47|     2.33| 1025.56|       2013-04-19 10:00:00|                  11.95|      rain|     13.12|    0.69|        2013-04-20 01:00:00|                  2.88|                 11.95|    4.0|2013-04-19 07:00:00|2013-04-20 03:04:14|          2.88|          6.17|          11.95|2013-04-19 12:57:01|2013-04-20 01:00:00|2013-04-19 20:00:00|Partly cloudy unt...|2013-04-20 13:00:00|                  3.88|       2013-04-20 01:00:00|       2013-04-20 13:00:00|     0.28|NULL|\n",
      "+----------+---------+--------------------+--------------------+--------------------+------------+--------------------+------------------+--------------------+--------------+-------------------+-----------+-----------------+--------+-------------------+----------+---------+--------+--------------------------+-----------------------+----------+----------+--------+---------------------------+----------------------+----------------------+-------+-------------------+-------------------+--------------+--------------+---------------+-------------------+-------------------+-------------------+--------------------+-------------------+----------------------+--------------------------+--------------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- day: date (nullable = true)\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- energy_median: double (nullable = true)\n",
      " |-- energy_mean: double (nullable = true)\n",
      " |-- energy_max: double (nullable = true)\n",
      " |-- energy_count: integer (nullable = true)\n",
      " |-- energy_std: double (nullable = true)\n",
      " |-- energy_sum: double (nullable = true)\n",
      " |-- energy_min: double (nullable = true)\n",
      " |-- temperatureMax: double (nullable = true)\n",
      " |-- temperatureMaxTime: timestamp (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- icon: string (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- temperatureMinTime: timestamp (nullable = true)\n",
      " |-- cloudCover: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperatureMinTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureHigh: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- apparentTemperatureHighTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureLow: double (nullable = true)\n",
      " |-- apparentTemperatureMax: double (nullable = true)\n",
      " |-- uvIndex: double (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- sunsetTime: timestamp (nullable = true)\n",
      " |-- temperatureLow: double (nullable = true)\n",
      " |-- temperatureMin: double (nullable = true)\n",
      " |-- temperatureHigh: double (nullable = true)\n",
      " |-- sunriseTime: timestamp (nullable = true)\n",
      " |-- temperatureHighTime: timestamp (nullable = true)\n",
      " |-- uvIndexTime: timestamp (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- temperatureLowTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureMin: double (nullable = true)\n",
      " |-- apparentTemperatureMaxTime: timestamp (nullable = true)\n",
      " |-- apparentTemperatureLowTime: timestamp (nullable = true)\n",
      " |-- moonPhase: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 02:25:07,023 INFO XGBoost-PySpark: _fit Running xgboost-2.1.4 on 1 workers with\n",
      "\tbooster params: {'objective': 'reg:squarederror', 'colsample_bytree': 0.85, 'device': 'cpu', 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.85, 'eta': 0.1, 'alpha': 0.5, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 600}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "2025-03-16 02:26:13,263 INFO XGBoost-PySpark: _train_booster Training on CPUs 1]\n",
      "[02:26:14] Task 0 got rank 0\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "2025-03-16 02:28:20,490 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n",
      "2025-03-16 02:29:27,541 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/caitlinyap/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE (XGBoost): 0.9680186443981336\n",
      "RMSE as percentage of mean: 9.56%\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StandardScaler\n",
    "from pyspark.sql.functions import col, dayofweek, when, month, hour, lag, avg, stddev, sum as spark_sum, expr\n",
    "from pyspark.sql.window import Window\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"EnergyPrediction\").config(\"spark.driver.memory\", \"12g\").config(\"spark.executor.memory\", \"6g\").getOrCreate()\n",
    "\n",
    "# Load Parquet file\n",
    "repo_root = os.getcwd()  \n",
    "parquet_path = os.path.join(repo_root, \"merged_df1_df3_df7_df8\")\n",
    "df = spark.read.parquet(parquet_path)\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "\n",
    "# Time-Based Features\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"day\")))\n",
    "df = df.withColumn(\"is_weekend\", when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), 1).otherwise(0))\n",
    "df = df.withColumn(\"month\", month(col(\"day\")))\n",
    "df = df.withColumn(\"hour\", hour(col(\"time\")))\n",
    "\n",
    "# Lag Features\n",
    "window_spec = Window.partitionBy(\"LCLid\").orderBy(\"day\")\n",
    "df = df.withColumn(\"lag_1_day\", lag(\"energy_sum\", 1).over(window_spec))\n",
    "df = df.withColumn(\"lag_2_day\", lag(\"energy_sum\", 2).over(window_spec))\n",
    "df = df.withColumn(\"lag_7_day\", lag(\"energy_sum\", 7).over(window_spec))\n",
    "\n",
    "# Rolling Aggregates\n",
    "rolling_window_3d = window_spec.rowsBetween(-2, 0)\n",
    "rolling_window_7d = window_spec.rowsBetween(-6, 0)\n",
    "rolling_window_14d = window_spec.rowsBetween(-13, 0)\n",
    "rolling_window_30d = window_spec.rowsBetween(-29, 0)\n",
    "\n",
    "df = df.withColumn(\"rolling_energy_3d\", spark_sum(\"energy_sum\").over(rolling_window_3d))\n",
    "df = df.withColumn(\"rolling_energy_7d\", spark_sum(\"energy_sum\").over(rolling_window_7d))\n",
    "df = df.withColumn(\"rolling_energy_14d\", spark_sum(\"energy_sum\").over(rolling_window_14d))\n",
    "df = df.withColumn(\"rolling_energy_30d\", spark_sum(\"energy_sum\").over(rolling_window_30d))\n",
    "\n",
    "df = df.withColumn(\"rolling_energy_mean_14d\", avg(\"energy_sum\").over(rolling_window_14d))\n",
    "df = df.withColumn(\"rolling_energy_std_14d\", stddev(\"energy_sum\").over(rolling_window_14d))\n",
    "\n",
    "# Energy Consumption Trends\n",
    "df = df.withColumn(\"energy_trend_3d\", col(\"rolling_energy_3d\") - col(\"lag_1_day\"))\n",
    "df = df.withColumn(\"energy_trend_7d\", col(\"rolling_energy_7d\") - col(\"lag_1_day\"))\n",
    "\n",
    "# Daily Energy Change Percentage\n",
    "df = df.withColumn(\"daily_energy_change\", (col(\"lag_1_day\") - col(\"energy_sum\")) / (col(\"lag_1_day\") + 1))\n",
    "\n",
    "# Weather Interactions\n",
    "df = df.withColumn(\"temperature_variability\", col(\"temperatureMax\") - col(\"temperatureMin\"))\n",
    "df = df.withColumn(\"humidity_temp_interaction\", col(\"humidity\") * col(\"temperatureMax\"))\n",
    "df = df.withColumn(\"cloud_temp_interaction\", col(\"cloudCover\") * col(\"temperatureMax\"))\n",
    "\n",
    "# Fill Missing Values\n",
    "df = df.dropna(subset=[\"energy_sum\"])  \n",
    "\n",
    "df = df.fillna({\n",
    "    \"lag_1_day\": 0, \"lag_2_day\": 0, \"lag_7_day\": 0,\n",
    "    \"rolling_energy_3d\": df.select(avg(\"energy_sum\")).collect()[0][0],\n",
    "    \"rolling_energy_7d\": df.select(avg(\"energy_sum\")).collect()[0][0],\n",
    "    \"rolling_energy_14d\": df.select(avg(\"energy_sum\")).collect()[0][0],\n",
    "    \"rolling_energy_30d\": df.select(avg(\"energy_sum\")).collect()[0][0],\n",
    "    \"rolling_energy_mean_14d\": df.select(avg(\"energy_sum\")).collect()[0][0],\n",
    "    \"rolling_energy_std_14d\": df.select(stddev(\"energy_sum\")).collect()[0][0],\n",
    "    \"energy_trend_3d\": 0, \"energy_trend_7d\": 0,\n",
    "    \"daily_energy_change\": 0,\n",
    "    \"temperature_variability\": df.select(avg(\"temperatureMax\") - avg(\"temperatureMin\")).collect()[0][0],\n",
    "    \"humidity_temp_interaction\": df.select(avg(\"humidity\") * avg(\"temperatureMax\")).collect()[0][0],\n",
    "    \"cloud_temp_interaction\": df.select(avg(\"cloudCover\") * avg(\"temperatureMax\")).collect()[0][0],\n",
    "})\n",
    "\n",
    "# Feature Engineering\n",
    "feature_cols = [\n",
    "    \"day_of_week\", \"is_weekend\", \"month\", \"hour\",\n",
    "    \"lag_1_day\", \"lag_2_day\", \"lag_7_day\",\n",
    "    \"rolling_energy_3d\", \"rolling_energy_7d\", \"rolling_energy_14d\", \"rolling_energy_30d\",\n",
    "    \"rolling_energy_mean_14d\", \"rolling_energy_std_14d\",\n",
    "    \"energy_trend_3d\", \"energy_trend_7d\",\n",
    "    \"daily_energy_change\",\n",
    "    \"temperature_variability\", \"humidity_temp_interaction\",\n",
    "    \"cloud_temp_interaction\", \n",
    "]\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "df = scaler.fit(df).transform(df)\n",
    "\n",
    "# Train-Test Split\n",
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "# Optimized XGBoost Model\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col=\"scaled_features\",\n",
    "    label_col=\"energy_sum\",\n",
    "    max_depth=5,  \n",
    "    eta=0.1,  \n",
    "    subsample=0.85,  \n",
    "    colsample_bytree=0.85,  \n",
    "    min_child_weight=5,  \n",
    "    alpha=0.5,  # L1 regularization\n",
    "    n_estimators=600\n",
    ")\n",
    "\n",
    "xgb_model = xgb.fit(train_df)\n",
    "predictions = xgb_model.transform(test_df)\n",
    "\n",
    "# Evaluate Model\n",
    "evaluator = RegressionEvaluator(labelCol=\"energy_sum\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Test RMSE (XGBoost): {rmse}\")\n",
    "\n",
    "mean_energy_sum = df.select(avg(\"energy_sum\")).collect()[0][0]\n",
    "rmse_percentage = (rmse / mean_energy_sum) * 100\n",
    "print(f\"RMSE as percentage of mean: {rmse_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reason for RMSE improvement:**\n",
    "\n",
    "**A. Better Feature Engineering**\n",
    "- Model views energy usage as a time series trend rather than isolated values\n",
    "1. Rolling Aggregates & Trends:\n",
    "    - Rolling energy sums over 3-day, 7-day, 14-day, and 30-day windows.\n",
    "2. Daily Energy Change Percentage\n",
    "    - Measured the daily change rate instead of just raw values\n",
    "    - Allowed the model to learn how energy consumption fluctuates over time\n",
    "3. Energy Trends Over Time\n",
    "    - energy_trend_3d, energy_trend_7d showed short-term and long-term variations.\n",
    "    - Clearer understanding of consumption trends\n",
    "\n",
    "**B. Hyperparameter Tuning in XGBoost**\n",
    "1. max_depth → 5 (previously 7-10)\n",
    "    - Prevented overfitting \n",
    "    - Shallower trees make better general predictions.\n",
    "2. eta (learning rate) → 0.1 (previously 0.2)\n",
    "    - Slower, more careful learning instead of rushing to fit data\n",
    "3. subsample & colsample_bytree → 0.85\n",
    "    - Forced better generalization by randomly selecting data points & features\n",
    "\n",
    "**C. Feature Scaling with StandardScaler**\n",
    "1. Switched from MinMaxScaler → StandardScaler\n",
    "    - MinMaxScaler squashes values between 0 and 1\n",
    "    - StandardScaler makes all features follow a normal distribution (mean=0, std=1)\n",
    "    - Prevented some features from dominating the model due to scale differences\n",
    "\n",
    "**D. Larger Boosting Rounds (600 Trees)**\n",
    "1. More iterations → Better learning\n",
    "    - Previous models ran with ~200 trees.\n",
    "    - Running 600 boosting rounds means the model learned more fine-grained details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
